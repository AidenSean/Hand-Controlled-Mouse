{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae965ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\users\\prith\\appdata\\roaming\\python\\python311\\site-packages (0.10.11)\n",
      "Requirement already satisfied: absl-py in d:\\anaconda\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in d:\\anaconda\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\anaconda\\lib\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: jax in c:\\users\\prith\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.26)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (from mediapipe) (3.7.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from mediapipe) (1.24.3)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\prith\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in d:\\anaconda\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\prith\\appdata\\roaming\\python\\python311\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in d:\\anaconda\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in d:\\anaconda\\lib\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in d:\\anaconda\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in d:\\anaconda\\lib\\site-packages (from jax->mediapipe) (1.11.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (10.0.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70d6f0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Set up webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and find hands\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Convert the image color back so it can be displayed\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Get the tip of the index finger\n",
    "            tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "            h, w, _ = image.shape\n",
    "            x, y = int(tip.x * w), int(tip.y * h)\n",
    "            \n",
    "            # Move the mouse\n",
    "            pyautogui.moveTo(x, y)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow('Finger Tracking', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c3ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "```python\n",
    "# ... [your existing code and imports] ...\n",
    "\n",
    "# Assume you have a variable for the previous y-position of the wrist for comparison\n",
    "previous_y_position = None\n",
    "\n",
    "# Inside your loop after you've calculated the index finger position\n",
    "wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "current_y_position = wrist.y\n",
    "\n",
    "# Scroll threshold could be how much change in y is needed to start scrolling\n",
    "scroll_threshold = 0.01  # Adjust based on sensitivity preference\n",
    "\n",
    "# Scroll speed could be how much you scroll\n",
    "scroll_speed = 5  # Adjust to preference\n",
    "\n",
    "# Determine scroll direction and execute\n",
    "if previous_y_position is not None:\n",
    "    y_change = current_y_position - previous_y_position\n",
    "    if y_change > scroll_threshold:\n",
    "        pyautogui.scroll(-scroll_speed)  # Scroll down\n",
    "    elif y_change < -scroll_threshold:\n",
    "        pyautogui.scroll(scroll_speed)  # Scroll up\n",
    "\n",
    "# Update the previous_y_position\n",
    "previous_y_position = current_y_position\n",
    "\n",
    "# ... [rest of your code] ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed1f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa4696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Set up webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,2560)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,1440)\n",
    "\n",
    "# Initialize a variable to track click state\n",
    "click_state = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display\n",
    "    # Convert the BGR image to RGB\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the image and find hands\n",
    "    results = hands.process(image)\n",
    "\n",
    "    # Convert the image color back so it can be displayed\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        #for hand_landmarks in results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            \n",
    "                # Assume you have a variable for the previous y-position of the wrist for comparison\n",
    "                previous_y_position = None\n",
    "\n",
    "                # Inside your loop after you've calculated the index finger position\n",
    "                wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "                current_y_position = wrist.y\n",
    "\n",
    "                # Scroll threshold could be how much change in y is needed to start scrolling\n",
    "                scroll_threshold = 0.01  # Adjust based on sensitivity preference\n",
    "\n",
    "                # Scroll speed could be how much you scroll\n",
    "                scroll_speed = 5  # Adjust to preference\n",
    "\n",
    "                # Determine scroll direction and execute\n",
    "                if previous_y_position is not None:\n",
    "                    y_change = current_y_position - previous_y_position\n",
    "                    if y_change > scroll_threshold:\n",
    "                        pyautogui.scroll(-scroll_speed)  # Scroll down\n",
    "                    elif y_change < -scroll_threshold:\n",
    "                        pyautogui.scroll(scroll_speed)  # Scroll up\n",
    "\n",
    "                # Update the previous_y_position\n",
    "                previous_y_position = current_y_position\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                # Get the tip of the index finger\n",
    "                tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                h, w, _ = image.shape\n",
    "                x, y = int(tip.x * w), int(tip.y * h)\n",
    "                \n",
    "\n",
    "\n",
    "                # Move the mouse\n",
    "                pyautogui.moveTo(x, y)\n",
    "\n",
    "                # Get the tips and bottoms of index and middle fingers\n",
    "                index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                index_bottom = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_DIP]\n",
    "                middle_bottom = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_DIP]\n",
    "\n",
    "                # Check if fingers are closed for a left click\n",
    "                if index_tip.y > index_bottom.y and middle_tip.y > middle_bottom.y:\n",
    "                    if click_state != 'left_click':\n",
    "                        pyautogui.click()\n",
    "                        click_state = 'left_click'\n",
    "                # Check if only index finger is extended for a right click\n",
    "                elif index_tip.y < index_bottom.y and middle_tip.y > middle_bottom.y:\n",
    "                    if click_state != 'right_click':\n",
    "                        pyautogui.rightClick()\n",
    "                        click_state = 'right_click'\n",
    "                else:\n",
    "                    click_state = None\n",
    "\n",
    "# Display the image\n",
    "    cv2.imshow('Finger Tracking', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c48e980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport cv2\\nimport mediapipe as mp\\nimport pyautogui\\nmp_hands = mp.solutions.hands\\nhands = mp_hands.Hands()\\nmp_drawing = mp.solutions.drawing_utils\\ncap = cv2.VideoCapture(0)\\ncap.set(cv2.CAP_PROP_FRAME_WIDTH,1920)\\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT,1080)\\nclick_state = None\\nwhile cap.isOpened():\\n    success, image = cap.read()\\n    if not success:\\n        continue\\n    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\\n    results = hands.process(image)\\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\\n    if results.multi_hand_landmarks:\\n            for hand_landmarks in results.multi_hand_landmarks:\\n                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\\n                previous_y_position = None\\n                wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\\n                current_y_position = wrist.y\\n                scroll_threshold = 0.01\\n                scroll_speed = 5\\n                if previous_y_position is not None:\\n                    y_change = current_y_position - previous_y_position\\n                    if y_change > scroll_threshold:\\n                        pyautogui.scroll(-scroll_speed)\\n                    elif y_change < -scroll_threshold:\\n                        pyautogui.scroll(scroll_speed)\\n                previous_y_position = current_y_position\\n                tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\\n                h, w, _ = image.shape\\n                x, y = int(tip.x * w), int(tip.y * h)\\n                pyautogui.moveTo(x, y)\\n                index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\\n                middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\\n                index_bottom = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_DIP]\\n                middle_bottom = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_DIP]\\n                if index_tip.y > index_bottom.y and middle_tip.y > middle_bottom.y:\\n                    if click_state != 'left_click':\\n                        pyautogui.click()\\n                        click_state = 'left_click'\\n                elif index_tip.y < index_bottom.y and middle_tip.y > middle_bottom.y:\\n                    if click_state != 'right_click':\\n                        pyautogui.rightClick()\\n                        click_state = 'right_click'\\n                else:\\n                    click_state = None\\n    cv2.imshow('Finger Tracking', image)\\n    if cv2.waitKey(5) & 0xFF == 27:\\n        break\\ncap.release()\\ncv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH,1920)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT,1080)\n",
    "click_state = None\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                previous_y_position = None\n",
    "                wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
    "                current_y_position = wrist.y\n",
    "                scroll_threshold = 0.01\n",
    "                scroll_speed = 5\n",
    "                if previous_y_position is not None:\n",
    "                    y_change = current_y_position - previous_y_position\n",
    "                    if y_change > scroll_threshold:\n",
    "                        pyautogui.scroll(-scroll_speed)\n",
    "                    elif y_change < -scroll_threshold:\n",
    "                        pyautogui.scroll(scroll_speed)\n",
    "                previous_y_position = current_y_position\n",
    "                tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                h, w, _ = image.shape\n",
    "                x, y = int(tip.x * w), int(tip.y * h)\n",
    "                pyautogui.moveTo(x, y)\n",
    "                index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "                middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "                index_bottom = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_DIP]\n",
    "                middle_bottom = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_DIP]\n",
    "                if index_tip.y > index_bottom.y and middle_tip.y > middle_bottom.y:\n",
    "                    if click_state != 'left_click':\n",
    "                        pyautogui.click()\n",
    "                        click_state = 'left_click'\n",
    "                elif index_tip.y < index_bottom.y and middle_tip.y > middle_bottom.y:\n",
    "                    if click_state != 'right_click':\n",
    "                        pyautogui.rightClick()\n",
    "                        click_state = 'right_click'\n",
    "                else:\n",
    "                    click_state = None\n",
    "    cv2.imshow('Finger Tracking', image)\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d9227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-v VIDEO]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\prith\\AppData\\Roaming\\jupyter\\runtime\\kernel-80a584ac-e825-452d-a912-d1d365edba7d.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python ball_tracking.py --video ball_tracking_example.mp4\n",
    "# python ball_tracking.py\n",
    "\n",
    "# import computer vision packages\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "\n",
    "# import keras packages\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# import statistic/data packages\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# import utility packages\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "\n",
    "characters = {0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9',\n",
    "10:'A', 11:'B', 12:'C', 13:'D', 14:'E', 15:'F', 16:'G', 17:'H', 18:'I', 19:'J',\n",
    "20:'K', 21:'L', 22:'M', 23:'N', 24:'O', 25:'P', 26:'Q', 27:'R', 28:'S', 29:'T',\n",
    "30:'U', 31:'V', 32:'W', 33:'X', 34:'Y', 35:'Z', 36:'a', 37:'b', 38:'c', 39:'d',\n",
    "40:'e', 41:'f', 42:'g', 43:'h', 44:'i', 45:'j', 46:'k', 47:'l', 48:'m', 49:'n',\n",
    "50:'o', 51:'p', 52:'q', 53:'r', 54:'s', 55:'t', 56:'u', 57:'v', 58:'w', 59:'x',\n",
    "60:'y', 61:'z'}\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-v\", \"--video\",\n",
    "    help=\"path to the (optional) video file\")\n",
    "#ap.add_argument(\"-b\", \"--buffer\", type=int, default=64,\n",
    "#    help=\"max buffer size\")\n",
    "#ap.add_argument(\"-vb\", \"--verbose\", help=\"increase output verbosity\", \n",
    "#    action=\"store_true\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# load keras model\n",
    "def load_model():\n",
    "    # Load trained model\n",
    "    #if args.verbose:\n",
    "    #    print(\"Loading cnn model from disk.............\", end=\"\")\n",
    "\n",
    "    # Load JSON model\n",
    "    json_file = open('model_saves/cnn_model.json', 'r')\n",
    "    model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    model = model_from_json(model_json)\n",
    "\n",
    "    # Load model weights\n",
    "    model.load_weights(\"model_saves/cnn_model_weights.h5\")\n",
    "    #if args.verbose:\n",
    "    #    print(\"...finished.\")\n",
    "    return model\n",
    "\n",
    "# predict letter given a model and image\n",
    "def predict_model(model, image):\n",
    "    prediction = model.predict(image.reshape(1,28,28,1))[0]\n",
    "    prediction = np.argmax(prediction)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "\n",
    "# used in calculating depth of object from camera\n",
    "def update_depth(obj_width, focal_len, width):\n",
    "    obj_depth = 0\n",
    "    if width == 0:\n",
    "        return obj_depth, 0\n",
    "    elif obj_width == 0 or focal_len == 0:\n",
    "        return obj_depth, 0\n",
    "    else:\n",
    "        obj_depth = obj_width * focal_len / width\n",
    "        #print(\"Width: \", width)\n",
    "        #print(\"Calculating depth...\")\n",
    "        #print(\"Depth: \", obj_depth, \"  delta: \", obj_depth-focal_len)\n",
    "    return obj_depth, obj_depth-focal_len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #if args.verbose:\n",
    "    #    print(\"Turned on: verbosity\")\n",
    "\n",
    "    # load keras model\n",
    "    model = load_model()\n",
    "\n",
    "    # define the lower and upper boundaries of the \"green\"\n",
    "    # ball in the HSV color space, then initialize the\n",
    "    # list of tracked points\n",
    "    greenLower = (29, 86, 6)\n",
    "    greenUpper = (64, 255, 255)\n",
    "    #pts = deque(maxlen=args[\"buffer\"])\n",
    "    pts = deque(maxlen=512)\n",
    "\n",
    "    # Whiteboard to overlay vector for Display\n",
    "    # whiteboard = np.zeros((480,640,3), dtype=np.uint8)\n",
    "\n",
    "    # Blackboard to overlay vector for Classification Input\n",
    "    blackboard = np.zeros((480,640,3), dtype=np.uint8)\n",
    "\n",
    "    # 5x5 kernel for erosion and dilation\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "\n",
    "    # if a video path was not supplied, grab the reference\n",
    "    # to the webcam\n",
    "    if not args.get(\"video\", False):\n",
    "        vs = VideoStream(src=0).start()\n",
    "\n",
    "    # otherwise, grab a reference to the video file\n",
    "    else:\n",
    "        vs = cv2.VideoCapture(args[\"video\"])\n",
    "\n",
    "    # allow the camera or video file to warm up\n",
    "    time.sleep(2.0)\n",
    "\n",
    "    point_index = 1\n",
    "\n",
    "    obj_width = 0\n",
    "    focal_len = 0\n",
    "    obj_depth = 0\n",
    "    count = 0\n",
    "    start = False\n",
    "\n",
    "    # keep looping\n",
    "    while True:\n",
    "        # grab the current frame\n",
    "        frame = vs.read()\n",
    "\n",
    "        # handle the frame from VideoCapture or VideoStream\n",
    "        frame = frame[1] if args.get(\"video\", False) else frame\n",
    "\n",
    "        # if we are viewing a video and we did not grab a frame,\n",
    "        # then we have reached the end of the video\n",
    "        if frame is None:\n",
    "            break\n",
    "\n",
    "        # resize the frame, blur it, and convert it to the HSV\n",
    "        # color space\n",
    "        frame = imutils.resize(frame, width=600)\n",
    "        blurred = cv2.GaussianBlur(frame, (11, 11), 0)\n",
    "        hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # construct a mask for the color \"green\", then perform\n",
    "        # a series of dilations and erosions to remove any small\n",
    "        # blobs left in the mask\n",
    "        '''\n",
    "        Leaving as is for now and will maybe update if results aren't good. \n",
    "        '''\n",
    "        mask = cv2.inRange(hsv, greenLower, greenUpper)\n",
    "        mask = cv2.erode(mask, None, iterations=2)\n",
    "        mask = cv2.dilate(mask, None, iterations=2)\n",
    "\n",
    "        # find contours in the mask and initialize the current\n",
    "        # (x, y) center of the ball\n",
    "        cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n",
    "            cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = imutils.grab_contours(cnts)\n",
    "        center = None\n",
    "        width = 0\n",
    "\n",
    "        do_save = False\n",
    "        \n",
    "        # only proceed if at least one contour was found\n",
    "        if len(cnts) > 0:\n",
    "            # find the largest contour in the mask, then use\n",
    "            # it to compute the minimum enclosing circle and\n",
    "            # centroid\n",
    "            c = max(cnts, key=cv2.contourArea)\n",
    "            ((x, y), radius) = cv2.minEnclosingCircle(c)\n",
    "            M = cv2.moments(c)\n",
    "            center = (int(M[\"m10\"] / M[\"m00\"]), int(M[\"m01\"] / M[\"m00\"]))\n",
    "\n",
    "            # only proceed if the radius meets a minimum size\n",
    "            if radius > 10:\n",
    "                \n",
    "                # recalculate\n",
    "                width = 2*radius\n",
    "                \n",
    "                # need to update parameters from zero value\n",
    "                obj_depth, delta = update_depth(obj_width, focal_len, width)\n",
    "                #print (\"delta : \", delta)\n",
    "                \n",
    "                '''\n",
    "                if delta > 1 or delta < -3:\n",
    "                    circle_status = (0, 10, 255) # too far/close, red\n",
    "                elif delta > 0.5 or delta < -2:\n",
    "                    circle_status = (0, 255, 255) # too close, yellow\n",
    "                '''\n",
    "                if delta > 1:\n",
    "                    circle_status = (0, 255, 255) # too close, yellow\n",
    "                elif delta < -3:\n",
    "                    circle_status = (0, 10, 255) # too far/close, red\n",
    "                else:\n",
    "                    circle_status = (10, 255, 10) # on plane, green\n",
    "                    do_save = True\n",
    "\n",
    "                # draw the circle and centroid on the frame,\n",
    "                # then update the list of tracked points\n",
    "                cv2.circle(frame, (int(x), int(y)), int(radius), circle_status, 2)\n",
    "                cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "\n",
    "\n",
    "            # update the points queue only if it is the correct depth\n",
    "            \n",
    "            #print (do_save)\n",
    "            # if false then empty deque so that it doesn't try to connect the last line? \n",
    "            \n",
    "            if start and do_save:\n",
    "                pts.appendleft(center)\n",
    "            else:\n",
    "                pts = deque(maxlen=512)\n",
    "\n",
    "        # loop over the set of tracked points\n",
    "        for i in range(1, len(pts)):\n",
    "            if pts[i - 1] is None or pts[i] is None:\n",
    "                continue\n",
    "            \n",
    "            cv2.line(frame, pts[i - 1], pts[i], (0, 0, 255), 8)\n",
    "            # EMNIST data is white characters on black\n",
    "            cv2.line(blackboard, pts[i - 1], pts[i], (255, 255, 255), 8)\n",
    "\n",
    "\n",
    "        # show the frame to our screen\n",
    "        mirrored_frame = frame.copy()\n",
    "        mirrored_frame = cv2.flip(frame, 1)\n",
    "        cv2.imshow(\"Frame\", mirrored_frame)\n",
    "\n",
    "        mirrored_board = blackboard.copy()\n",
    "        mirrored_board = cv2.flip(blackboard, 1)\n",
    "        cv2.imshow(\"Blackboard\", mirrored_board)\n",
    "        \n",
    "        '''\n",
    "        #cv2.imwrite(\"blackboardimage.png\", mirrored_board)\n",
    "        blackboard_gray = cv2.cvtColor(mirrored_board, cv2.COLOR_BGR2GRAY)\n",
    "        #cv2.imwrite(\"blackboard_gray_image.png\", blackboard_gray)\n",
    "        blur1 = cv2.medianBlur(blackboard_gray, 15)\n",
    "        blur1 = cv2.GaussianBlur(blur1, (5, 5), 0)\n",
    "        #cv2.imwrite(\"blurredimage.png\", blur1)\n",
    "        ret, thresh = cv2.threshold(blur1, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        # Finding contours on the blackboard\n",
    "        blackboard_img, blackboard_cnts, blackboard_hier = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        #cv2.imwrite(\"contourblackboard.png\", blackboard_img)\n",
    "        \n",
    "        if len(blackboard_cnts) > 0:\n",
    "            cnt = sorted(blackboard_cnts, key = cv2.contourArea, reverse = True)[0]\n",
    "            if cv2.contourArea(cnt) > 1000:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                alphabet = blackboard_gray[y-10:y + h + 10, x-10:x + w + 10]\n",
    "                newImage = cv2.resize(alphabet, (28, 28))\n",
    "                \n",
    "                path = 'input_images/'\n",
    "                cv2.imwrite(os.path.join(path, \"img-%d.png\"%count), newImage)\n",
    "        '''\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the 'q' key is pressed, stop the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        # if the 's' key is pressed, consider this the \"start size\"\n",
    "        if key == ord(\"s\"):\n",
    "            obj_width = width\n",
    "            focal_len = 10\n",
    "            start = True\n",
    "            #if args.verbose:\n",
    "            #    print(\"Tracker width: \", obj_width)\n",
    "            #    print(\"Whiteboard depth: \", focal_len)\n",
    "\n",
    "        # save the image, erase blackboard, empty saved points queue\n",
    "        # immediately begin writing next letter\n",
    "        if key == ord(\"d\"):\n",
    "            #cv2.imwrite(\"blackboardimage.png\", mirrored_board)\n",
    "            blackboard_gray = cv2.cvtColor(mirrored_board, cv2.COLOR_BGR2GRAY)\n",
    "            #cv2.imwrite(\"blackboard_gray_image.png\", blackboard_gray)\n",
    "            blur1 = cv2.medianBlur(blackboard_gray, 15)\n",
    "            blur1 = cv2.GaussianBlur(blur1, (5, 5), 0)\n",
    "            #cv2.imwrite(\"blurredimage.png\", blur1)\n",
    "            ret, thresh = cv2.threshold(blur1, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "            # Finding contours on the blackboard\n",
    "            blackboard_img, blackboard_cnts, blackboard_hier = cv2.findContours(thresh.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "            #cv2.imwrite(\"contourblackboard.png\", blackboard_img)\n",
    "            \n",
    "            if len(blackboard_cnts) > 0:\n",
    "                cnt = sorted(blackboard_cnts, key = cv2.contourArea, reverse = True)[0]\n",
    "                if cv2.contourArea(cnt) > 1000:\n",
    "                    x, y, w, h = cv2.boundingRect(cnt)\n",
    "                    alphabet = blackboard_gray[y-10:y + h + 10, x-10:x + w + 10]\n",
    "                    newImage = cv2.resize(alphabet, (28, 28))\n",
    "                    \n",
    "                    # predict char digit\n",
    "                    print(characters[predict_model(model, newImage)])\n",
    "\n",
    "                    # save image to disk\n",
    "                    path = 'input_images/'\n",
    "                    cv2.imwrite(os.path.join(path, \"img-%d.png\"%count), newImage)\n",
    "\n",
    "            count += 1 \n",
    "            blackboard = np.zeros((480, 640, 3), dtype=np.uint8)    \n",
    "            pts = deque(maxlen=512)\n",
    "\n",
    "        '''\n",
    "        # depth testing\n",
    "        if key == ord(\"c\"):\n",
    "            if width == 0:\n",
    "                print(\"Can not find tracker.\")\n",
    "            elif obj_width == 0 or focal_len == 0:\n",
    "                print(\"Whiteboard not yet set.\")\n",
    "            else:\n",
    "                obj_depth = update_depth(obj_depth, focal_len, width)\n",
    "        '''\n",
    "\n",
    "\n",
    "    # if we are not using a video file, stop the camera video stream\n",
    "    if not args.get(\"video\", False):\n",
    "        vs.stop()\n",
    "\n",
    "    # otherwise, release the camera\n",
    "    else:\n",
    "        vs.release()\n",
    "\n",
    "    # close all windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fcbc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a801da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
